#!pip install optuna


#importing libraries and packages 
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \
                            precision_score, recall_score, f1_score, roc_auc_score, \
                            roc_curve, confusion_matrix, log_loss
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import cross_validate, cross_val_score, KFold

import optuna


#data
df = pd.read_csv('../artifact/cirrhosis.csv')
df.head()


df.info()


df.columns


df.drop_duplicates()


df.describe().T


df.isnull().sum()





for i in df.columns.values.tolist():
    print(i,'->', df[i].value_counts().sum())#, df.i.unique())


df['Stage'].value_counts()


sns.countplot(y=df['Stage'])


# categorical
cat = df.select_dtypes(include='object')
cat.columns


# stage visualoze
plt.figure(figsize=(21.2,10))

plt.subplot(2,3,1)
sns.countplot(x=df['Stage'], hue=df['Drug'])
plt.title('Medications v/s Stages')

plt.subplot(2,3,2)
sns.countplot(x=df['Stage'], hue=df['Sex'])
plt.title('Stage v/s Gender')

plt.subplot(2,3,3)
sns.countplot(x=df['Stage'], hue=df['Ascites'])
plt.title('Ascites proportion v/s Stages')

plt.subplot(2,3,4)
sns.countplot(x=df['Stage'], hue=df['Hepatomegaly'])
plt.title('Hepatomegaly')

plt.subplot(2,3,5)
sns.countplot(x=df['Stage'], hue=df['Spiders'])
plt.title('Presence of Spiders v/s Stages');

plt.subplot(2,3,6)
sns.countplot(x=df['Stage'], hue=df['Edema'])
plt.title('Edema');


# numerical
num = df.select_dtypes(include='number')
num.columns


#histplot Polts
plt.figure(figsize=(20.6,15))

plt.subplot(3,3,1)
sns.histplot(data =df['Age'], kde=True)

plt.subplot(3,3,2)
sns.histplot(data = df['Bilirubin'], kde=True)

plt.subplot(3,3,3)
sns.histplot(data=df['Cholesterol'],kde=True)

plt.subplot(3,3,4)
sns.histplot(data =df['Albumin'], kde=True)

plt.subplot(3,3,5)
sns.histplot(data =df['Copper'], kde=True)

plt.subplot(3,3,6)
sns.histplot(data =df['Alk_Phos'], kde=True)

plt.subplot(3,3,7)
sns.histplot(data = df['SGOT'],kde=True)

plt.subplot(3,3,8)
sns.histplot(data =df['Tryglicerides'], kde=True)

plt.subplot(3,3,9)
sns.histplot(data =df['Platelets'], kde=True)


sns.histplot(data =df['Prothrombin'], kde=True)





# convert age
df['Age']= (df['Age']/365.25).round()


df['Status'].value_counts()


# status: 0 = D (death), 1 = C (censored), 2 = CL (censored due to liver transplantation)

df['Status'] = df['Status'].map({'C': 1, 'D': 0, 'CL': 2})
df['Status'].value_counts()


df


for feature in cat:
    df[feature]=df[feature].fillna(df[feature].mode()[0])
df


for i in num:
    df[i]=round(df[i].fillna(df[i].mean()),2)
df


# numerical_col = ['Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',
#        'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']



# scaler = StandardScaler()

# df[numerical_col]=scaler.fit_transform(df[numerical_col])
# df


df.isnull().sum()


# Label encoder
le = LabelEncoder()
for i in cat:
    df[i]=le.fit_transform(df[i])

df


plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), annot=True, cmap='Reds')
plt.show()


#save to csv
new = df.copy()
new.to_csv('../artifact/process.csv')


new





#Creating Features and scores
x = df.drop(['Status','ID'], axis=1)
y = df['Status']


# hybrid: SMOTE + tomek link
# technique to combine both undersampling and oversampling
# SMOTE Technique:
from imblearn.combine import SMOTETomek
smote = SMOTETomek()
x_smote, y_smote = smote.fit_resample(x,y)


from collections import Counter
print('Before SMOTE : ', Counter(y))
print('After SMOTE  : ', Counter(y_smote))


# scaler =  StandardScaler()
# x  = scaler.fit_transform(x)
# x


#separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(x_smote,y_smote,test_size=0.2,random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "K-Neighbors Classifier": KNeighborsClassifier(),
    "XGBClassifier": XGBClassifier(),
    "LogisticRegression": LogisticRegression(),
    "CatBoosting Classifier": CatBoostClassifier(verbose=False),
    "Support Vector Classifier": SVC(probability=True),
    "AdaBoost Classifier": AdaBoostClassifier(),
    "Lightgbm Classifier": LGBMClassifier()

}


def evaluate_clf(true, predicted):
    acc = accuracy_score(true, predicted,) # Calculate Accuracy
    f1 = f1_score(true, predicted,average='micro') # Calculate F1-score
    precision = precision_score(true, predicted, average='micro') # Calculate Precision
    recall = recall_score(true, predicted, average='micro')  # Calculate Recall
    return acc, f1 , precision, recall


accuracy_list = []
model_name=[]
lloss=[]
probas=[]
for name, model in models.items():
    model_name.append(name)
    model.fit(X_train, y_train) # Train model
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    y_pred_prob = cross_val_predict(model,X_train, y_train, method='predict_proba', cv=5)
    probas.append(y_pred_prob)
    
    # # Training set performance
    # acc = round(accuracy_score(y_train, y_train_pred),2) # Calculate Accuracy
    # f1 = round(f1_score(y_train, y_train_pred,average='micro'),2) # Calculate F1-score
    # precision = precision_score(y_train, y_train_pred,average='micro') # Calculate Precision
    # recall = recall_score(y_train, y_train_pred,average='micro')  # Calculate Recall
    # print(name, acc,f1,precision,recall)

    # # Test set performance
    # acctest = round(accuracy_score(y_test, y_test_pred),2) # Calculate Accuracy
    # f1test = round(f1_score(y_test, y_test_pred,average='micro'),2) # Calculate F1-score
    # precisiontest = precision_score(y_test, y_test_pred,average='micro') # Calculate Precision
    # recalltest = recall_score(y_test, y_test_pred,average='micro')  # Calculate Recall
    # print(name, acctest,f1test,precisiontest,recalltest)
    logloss=log_loss(y_train, y_pred_prob, labels=model.classes_)
    lloss.append(logloss)

    # Training set performance
    model_train_accuracy, model_train_f1,model_train_precision,\
    model_train_recall=evaluate_clf(y_train, y_train_pred)


    # Test set performance
    model_test_accuracy,model_test_f1,model_test_precision,\
    model_test_recall=evaluate_clf(y_test, y_test_pred)

    print('Model performance for Training set',name)
    print("- Accuracy: {:.4f}".format(model_train_accuracy))
    print('- F1 score: {:.4f}'.format(model_train_f1)) 
    print('- Precision: {:.4f}'.format(model_train_precision))
    print('- Recall: {:.4f}'.format(model_train_recall))

    print('----------------------------------')

    print('Model performance for Test set',name)
    print('- Accuracy: {:.4f}'.format(model_test_accuracy))
    accuracy_list.append(model_test_accuracy)
    print('- F1 score: {:.4f}'.format(model_test_f1))
    print('- Precision: {:.4f}'.format(model_test_precision))
    print('- Recall: {:.4f}'.format(model_test_recall))

    # print('- Proba: ', y_pred_prob)
    print('- logloss: {:.4f}'.format(logloss))
          
    print('='*35)
    print('\n')
 
    


report=pd.DataFrame(list(zip(model_name, accuracy_list, lloss)), columns=['Model Name', 'Accuracy','logloss']).sort_values(by=['Accuracy'], ascending=False)
report






# 1. Gradient Boosting Classifier
gbc = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.05,
                                 random_state=42,
                                 max_features=5 )
# Fit to training set
gbc.fit(X_train,y_train)
 
# Predict on test set
y_pred = gbc.predict(X_test)
 
# accuracy
acc = accuracy_score(y_test, y_pred)
print("Gradient Boosting Classifier accuracy is : {}".format(acc))
print(classification_report(y_test, y_pred))


# 2. GridSearchCV Gradient boosting
param_grid = {
    "learning_rate": [0.01, 0.1, 1],
    "max_depth":[3,5,8],
    "n_estimators":[10, 50, 100, 200]
    }

# Initialize the Gradient Boosting model
gb_model = GradientBoostingClassifier()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1)#, scoring='accuracy')

# Fit the model to the training data using GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best parameters, best model and score
print('best_params: ',grid_search.best_params_)
print('best_model: ', grid_search.best_estimator_)
print('score: ', grid_search.score(X_train, y_train))

# Make predictions on the test set using the best model
y_pred_best = grid_search.predict(X_test)

# Evaluate the best model
accuracy_best = accuracy_score(y_test, y_pred_best)
print("GridSearch Gradient Test Accuracy:", accuracy_best)
print(classification_report(y_test, y_pred_best))


# 3. Gradient Boosting Classifier-SMOTE-StandardScaler

x = new.drop(['Status','ID'], axis=1)
y = new['Status']
smote = SMOTETomek()
x_smote, y_smote = smote.fit_resample(x,y)
scaler =  StandardScaler()
x  = scaler.fit_transform(x_smote)


X_train, X_test, y_train, y_test = train_test_split(x,y_smote,test_size=0.2,random_state=42)

gbcsmote = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.05,
                                 random_state=42,
                                 max_features=5 )
# Fit to training set
gbcsmote.fit(X_train,y_train)
 
# Predict on test set
y_pred_smote = gbcsmote.predict(X_test)
 
# accuracy
acc = accuracy_score(y_test, y_pred_smote)
print("SMOTE Gradient Boosting Classifier accuracy is : {}".format(acc))
print(classification_report(y_test, y_pred_smote))


# 4. GridSearchCV Gradient Boosting Classifier-SMOTE-StandardScaler

x = new.drop(['Status','ID'], axis=1)
y = new['Status']
smote = SMOTETomek()
x_smote, y_smote = smote.fit_resample(x,y)
scaler =  StandardScaler()
x  = scaler.fit_transform(x_smote)


X_train, X_test, y_train, y_test = train_test_split(x,y_smote,test_size=0.2,random_state=42)

param_grid = {
    "learning_rate": [0.01, 0.1, 1],
    "max_depth":[3,5,8],
    "n_estimators":[10, 50, 100, 200]
    }

# Initialize the Gradient Boosting model
gb_model = GradientBoostingClassifier()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1)#, scoring='accuracy')

# Fit the model to the training data using GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best parameters, best model and score
print('best_params: ',grid_search.best_params_)
print('best_model: ', grid_search.best_estimator_)
print('score: ', grid_search.score(X_train, y_train))

# Make predictions on the test set using the best model
y_pred_best = grid_search.predict(X_test)

# Evaluate the best model
accuracy_best = accuracy_score(y_test, y_pred_best)
print("SMOTE Gradient GrideSearch Test Accuracy:", accuracy_best)
print(classification_report(y_test, y_pred_best))


#Hypertunning Optuna normal
def objective(trial):

    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        # 'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        # 'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        # 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.1, 1.0),
    }

    model = GradientBoostingClassifier(**params, random_state=42)

    model.fit(X_train, y_train)

    prediction_proba = model.predict_proba(X_test)

    loss = log_loss(y_test, prediction_proba)
    # Compute scores
    scores = cross_validate(model, x, y, cv = 5,  n_jobs = -1)#, scoring = scoring,)
    accuracy = scores["test_score"].mean()

    return accuracy

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print('Best trial:')
trial = study.best_trial

print('Value: {}'.format(trial.value))
print('Params: ')
for key, value in trial.params.items():
    print('{}: {}'.format(key, value))


#optuna
optuna.visualization.plot_optimization_history(study)





def model_evaluate(new):
    x = data.drop(['Status','ID'], axis=1)
    y = data['Status']

    gbc = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.05,
                                 random_state=100,
                                 max_features=5 )
	
    crossval_score = cross_val_score(gbc, x, y, cv=5, scoring='accuracy').mean()

    return crossval_score





new.iloc[300][::]


new.loc[300].values


new['Status'].iloc[300]


x = new.drop(['Status','ID'], axis=1)
y = new['Status']


y.iloc[300]


testset =  x.loc[300].values
print(testset)
test1=testset.reshape(1, -1)
test1


# gbc, gbcsmote, grid_search_model


gbc.predict(test1)[0]



